{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## WFP-01-03-02 CHIRPS Rainfall Estimates (RFE) - Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This application generates Rainfall Estimates (RFE) aggregations, from CHIRPS RFE 5km resolution, compared to a reference period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <a name=\"objective\">Objective \n",
    "\n",
    "The objective of this code is to determine:\n",
    "    - Sum of daily data over the past N days, derived every 10 days (N = 10, 30, 60, 90, 120, 150, 180, 270, 365 days)\n",
    "    - Counts of daily data above 1mm over the past N days, derived every 10 days (N = 30, 60, 90 days).\n",
    "    - Longest sequence of daily values < 2mm (\"dry spell\") within the last N days, derived every 10 days (N = 30, 60, 90 days).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"service\">Service definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = dict([('title', 'CHIRPS Rainfall Estimates (RFE) - Aggregations'),\n",
    "                ('abstract', 'TBD'),\n",
    "                ('id', 'wfp-01-03-02')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"runtime\">Runtime parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input references**\n",
    "\n",
    "This is the CHIRPS stack catalogue references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_references = 'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.03.10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_10 = dict([('id', 'N_10'),\n",
    "                          ('value', 'True'),\n",
    "                          ('title', '10 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 10 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "N_30 = dict([('id', 'N_30'),\n",
    "                          ('value', 'True'),\n",
    "                          ('title', '30 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 30 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_60 = dict([('id', 'N_60'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '60 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 60 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_90 = dict([('id', 'N_90'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '90 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 90 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_120 = dict([('id', 'N_120'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '120 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 120 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_150 = dict([('id', 'N_150'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '150 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 150 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_180 = dict([('id', 'N_180'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '180 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 180 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_270 = dict([('id', 'N_270'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '270 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 270 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_365 = dict([('id', 'N_365'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '365 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 365 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionOfInterest = dict([('id', 'regionOfInterest'),\n",
    "                          ('value', 'POLYGON((-30 -10, 20 -10, 20 40, -30 40, -30 -10))'),\n",
    "                          ('title', 'WKT Polygon for the Region of Interest'),\n",
    "                          ('abstract', 'Set the value of WKT Polygon')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data path**\n",
    "\n",
    "This path defines where the data is staged-in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/workspace/data/chirps-2.0/\"\n",
    "unzipped_chirps_path = \"/workspace/data/chirps-2.0/unzipped_chirps/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"workflow\">Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the packages required for processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/opt/anaconda/lib/python27.zip', '/opt/anaconda/lib/python2.7', '/opt/anaconda/lib/python2.7/plat-linux2', '/opt/anaconda/lib/python2.7/lib-tk', '/opt/anaconda/lib/python2.7/lib-old', '/opt/anaconda/lib/python2.7/lib-dynload', '/opt/anaconda/lib/python2.7/site-packages/Glymur-0.8.6-py2.7.egg', '/opt/anaconda/lib/python2.7/site-packages/click_plugins-1.0.3-py2.7.egg', '/opt/anaconda/lib/python2.7/site-packages/setuptools-23.0.0-py2.7.egg', '/opt/anaconda/lib/python2.7/site-packages', '/opt/anaconda/lib/python2.7/site-packages/PIL', '/opt/anaconda/lib/python2.7/site-packages/IPython/extensions', '/home/rirr/.ipython', '/workspace/wfp-01-03-02/src/main/app-resources/notebook/libexec', '/application/notebook/libexec/']\n"
     ]
    }
   ],
   "source": [
    "from osgeo import gdal, ogr, osr\n",
    "from geopandas import GeoDataFrame\n",
    "import gzip\n",
    "import cioppy\n",
    "import shutil\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append('/application/notebook/libexec/')\n",
    "from aux_functions import matrix_sum, mask_matrix, crop_image, write_output_image\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(row, search_params):\n",
    "    search = ciop.search(end_point=row['catalogue_url'], \n",
    "                                  params=search_params,\n",
    "                                  output_fields='identifier,startdate,enclosure',\n",
    "                                  model='GeoTime')[0]\n",
    "    \n",
    "    series = pd.Series(search)\n",
    "    \n",
    "    series['startdate'] = pd.to_datetime(series['startdate'])\n",
    "    \n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-03-10\n",
      "2017-03-10\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enclosure</th>\n",
       "      <th>identifier</th>\n",
       "      <th>startdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ftp://anonymous@ftp.chg.ucsb.edu/pub/org/chg/p...</td>\n",
       "      <td>chirps-v2.0.2017.03.10</td>\n",
       "      <td>2017-03-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           enclosure              identifier  \\\n",
       "0  ftp://anonymous@ftp.chg.ucsb.edu/pub/org/chg/p...  chirps-v2.0.2017.03.10   \n",
       "\n",
       "   startdate  \n",
       "0 2017-03-10  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(input_references, str):\n",
    "    input_references = [input_references]\n",
    "\n",
    "\n",
    "'''test_references = ['https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.03', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.02',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.01', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.05',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.04', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.06',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.07', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.08',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.09', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.10',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.11', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.12',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.13', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.14',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.15', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.16',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.17', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.18',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.19', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.20',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.21', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.22',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.23', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.24',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.25', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.26',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.27', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.28',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.29', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.30']\n",
    "                  # 'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.05.31']\n",
    "''''''\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.03', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.02',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.01', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.05',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.04', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.06',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.07', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.08',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.09', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.10',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.11', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.12',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.13', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.14',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.15', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.16',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.17', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.18',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.19', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.20',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.21', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.22',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.23', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.24',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.25', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.26',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.27', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.28',\n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.29', \n",
    "                   'https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.04.30']\n",
    "'''\n",
    "\n",
    "ciop = cioppy.Cioppy()\n",
    "\n",
    "gpd_data = GeoDataFrame(input_references,\n",
    "                       columns=['catalogue_url'])\n",
    "\n",
    "region_of_interest = regionOfInterest['value']\n",
    "gpd_data = gpd_data.sort_values(by='catalogue_url')\n",
    "start_date = re.findall('\\d{4}\\.\\d{2}\\.\\d{2}', gpd_data.iloc[0]['catalogue_url'])[0].replace('.', '-')\n",
    "end_date = re.findall('\\d{4}\\.\\d{2}\\.\\d{2}', gpd_data.iloc[-1]['catalogue_url'])[0].replace('.', '-')\n",
    "print(start_date)\n",
    "print(end_date)\n",
    "print(len(input_references))\n",
    "search_params =  dict([('start', start_date),\n",
    "                      ('stop', end_date),\n",
    "                      ('count', len(input_references))])\n",
    "gpd_final = gpd_data.apply(lambda row: get_info(row, search_params), axis=1)\n",
    "gpd_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_aggregations(product_list, N_value, region_of_interest):\n",
    "    mask_no_data_value = 0\n",
    "    sum_result = 0\n",
    "    count_above_one = 0\n",
    "    max_sequence = 0\n",
    "    temp_mat = 0\n",
    "    regions_below_two = 0\n",
    "    projection = None\n",
    "    geo_transform = None\n",
    "    print(type(product_list))\n",
    "    for chirp_product_url in product_list:\n",
    "        # uncompressed data\n",
    "        chirp_product = (chirp_product_url.split('/')[-1]).split('.gz')[0]\n",
    "        print(chirp_product)\n",
    "        cropped_product_path = 'crop_' + chirp_product\n",
    "        try:\n",
    "            crop_image(chirp_product_url, region_of_interest, cropped_product_path)\n",
    "            # Read GeoTIFF as an array\n",
    "            dataset = gdal.Open(cropped_product_path)\n",
    "            product_array = dataset.GetRasterBand(1).ReadAsArray()\n",
    "            no_data_value = dataset.GetRasterBand(1).ComputeRasterMinMax()[0]\n",
    "            geo_transform = dataset.GetGeoTransform()\n",
    "            projection = dataset.GetProjection()\n",
    "            ## Create mask of no_data_values\n",
    "            if isinstance(mask_no_data_value, int):\n",
    "                mask_no_data_value = np.where(product_array == no_data_value, 1, 0)\n",
    "            else:\n",
    "                temp_mask = np.where(product_array == no_data_value, 1, 0)\n",
    "                mask_no_data_value = matrix_sum(mask_no_data_value, temp_mask)\n",
    "\n",
    "            ## Create iteratively the sum array\n",
    "            sum_result = matrix_sum(sum_result, product_array, no_data_value)\n",
    "\n",
    "            ## Create iteratively the array with the counts of daily data above 1mm\n",
    "            if N_value == 30 or N_value == 60 or N_value == 90:\n",
    "                regions_above_one = mask_matrix(product_array, 1, True, no_data_value)\n",
    "                count_above_one = matrix_sum(count_above_one, regions_above_one)\n",
    "\n",
    "                ## Create iteratively the array with the longest sequence of daily values <2mm\n",
    "                regions_below_two = mask_matrix(product_array, 2, False, no_data_value)\n",
    "                temp_mat = matrix_sum(temp_mat, regions_below_two)\n",
    "                if isinstance(max_sequence, int):\n",
    "                    max_sequence = temp_mat\n",
    "                max_sequence[regions_below_two == 0] = np.maximum(max_sequence[regions_below_two == 0], temp_mat[regions_below_two == 0])\n",
    "                temp_mat[regions_below_two == 0] = 0\n",
    "            dataset = None\n",
    "        except Exception as e:\n",
    "            print('Error processing the product ' + chirp_product + ': ' + str(e))\n",
    "        if os.path.exists(cropped_product_path):\n",
    "            os.remove(cropped_product_path)\n",
    "        \n",
    "    return sum_result, count_above_one, max_sequence, mask_no_data_value, projection, geo_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_outputs(first_date, last_date, sum_result, count_above_one, max_sequence, mask_no_data_value, image_format, product_count, projection, geo_transform):\n",
    "    write_output_image('CHIRPSv2.N' + str(product_count) + '.days_total.' + first_date + '.' + last_date + '.tif', sum_result, image_format, projection, geo_transform, mask_no_data_value)\n",
    "    if product_count == 30 or product_count == 60 or product_count == 90:\n",
    "        write_output_image('CHIRPSv2.N' + str(product_count) + '.count_above_one.' + first_date + '.' + last_date + '.tif', count_above_one, image_format, projection, geo_transform, mask_no_data_value)\n",
    "        write_output_image('CHIRPSv2.N' + str(product_count) + '.dry_spell.' + first_date + '.' + last_date + '.tif', max_sequence, image_format, projection, geo_transform, mask_no_data_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = [N_10['value'], N_30['value'], N_60['value'], N_90['value'], N_120['value'], N_150['value'], N_180['value'], N_270['value'], N_365['value']]\n",
    "nvalue = [10, 30, 60, 90, 120, 150, 180, 270, 365]\n",
    "nlist = [n=='True' for n in nlist]\n",
    "product_years = gpd_final['startdate'].dt.year.unique()\n",
    "for _year in product_years:\n",
    "    products_data = gpd_final[(gpd_final['startdate'].dt.year == _year)]\n",
    "    L = len(products_data.index.values)\n",
    "    months_of_products = products_data['startdate'].dt.month.unique()\n",
    "    for n in [index for index, value in enumerate(nlist) if value==True]:\n",
    "        N = nvalue[n]\n",
    "        n_months = N/30\n",
    "        if n_months > len(months_of_products):\n",
    "            print('Not enough products available for a aggregation of ' + str(N))\n",
    "            break\n",
    "        if N == 10:\n",
    "            #n_intervals=int(math.ceil((L*1.00)/N))\n",
    "            n_intervals=len(months_of_products)*3\n",
    "            for i in range(n_intervals):\n",
    "                print('Interval ' + str(i+1) + ' of N=10')\n",
    "                month_index = i/3\n",
    "                if i%3 == 0: #start of the month.\n",
    "                    start_date = products_data[((products_data['startdate'].dt.month == months_of_products[month_index]))]['startdate'].tolist()[0]\n",
    "                    end_date = products_data[((products_data['startdate'].dt.month == months_of_products[month_index]) & (products_data['startdate'].dt.day == 10))]['startdate'].tolist()[0]\n",
    "                elif i%3 == 2: #last interval of month. Each month has 3 N=10 aggregations\n",
    "                    start_date = products_data[((products_data['startdate'].dt.month == months_of_products[month_index]) & (products_data['startdate'].dt.day == 21))]['startdate'].tolist()[0]\n",
    "                    end_date = products_data[((products_data['startdate'].dt.month == months_of_products[month_index]))]['startdate'].tolist()[-1]\n",
    "                else:\n",
    "                    start_date = products_data[((products_data['startdate'].dt.month == months_of_products[month_index]) & (products_data['startdate'].dt.day == 11))]['startdate'].tolist()[0]\n",
    "                    end_date = products_data[((products_data['startdate'].dt.month == months_of_products[month_index]) & (products_data['startdate'].dt.day == 20))]['startdate'].tolist()[0]\n",
    "                interval_products = products_data[(products_data['startdate'] >= start_date) & (products_data['startdate'] <= end_date)]['enclosure'].tolist()\n",
    "                first_date = start_date.strftime('%Y.%m.%d')\n",
    "                last_date = end_date.strftime('%Y.%m.%d')\n",
    "                print(first_date)\n",
    "                print(last_date)\n",
    "                daily_sum, count_above_one, longest_sequence, no_value, projection, geo_transform = calc_aggregations(interval_products, N, region_of_interest)\n",
    "                write_outputs(first_date, last_date, daily_sum, count_above_one, longest_sequence, no_value, 'GTiff', N, projection, geo_transform)\n",
    "        else:\n",
    "            n_intervals = int(round((L*1.00)/N))\n",
    "            for i in range(0, n_intervals, n_months):\n",
    "                print('Interval ' + str(i+1) + ' of N ' + str(N))\n",
    "                print(months_of_products[i])\n",
    "                start_date = products_data[((products_data['startdate'].dt.month == months_of_products[i]))]['startdate'].tolist()[0]\n",
    "                end_date = products_data[((products_data['startdate'].dt.month == months_of_products[i]+ n_months-1))]['startdate'].tolist()[-1]\n",
    "                interval_products = products_data[(products_data['startdate'] >= start_date) & (products_data['startdate'] <= end_date)]['enclosure'].tolist()\n",
    "                first_date = start_date.strftime('%Y.%m.%d')\n",
    "                last_date = end_date.strftime('%Y.%m.%d')\n",
    "                print(first_date)\n",
    "                print(last_date)\n",
    "                daily_sum, count_above_one, longest_sequence, no_value, projection, geo_transform = calc_aggregations(interval_products, N, region_of_interest)\n",
    "                write_outputs(first_date, last_date, daily_sum, count_above_one, longest_sequence, no_value, 'GTiff', N, projection, geo_transform)\n",
    "            #interval_start = i*N\n",
    "            # interval_end = ((i+1)*N)\n",
    "            # if interval_end > L:\n",
    "            #     interval_end = L\n",
    "\n",
    "            #interval_products = products_data.iloc[interval_start:interval_end]['enclosure'].tolist()\n",
    "            #first_date = month_data.iloc[interval_start:interval_end]['startdate'].tolist()[0].strftime('%Y.%m.%d')\n",
    "            #last_date = month_data.iloc[interval_start:interval_end]['startdate'].tolist()[-1].strftime('%Y.%m.%d')\n",
    "           \n",
    "        #daily_sum, count_above_one, longest_sequence, no_value, projection, geo_transform = calc_aggregations(interval_products, N, region_of_interest)\n",
    "        #write_outputs(first_date, last_date, daily_sum, count_above_one, longest_sequence, no_value, 'GTiff', N, projection, geo_transform)     "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Read the products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "fig0 = pyplot.figure()\n",
    "ax0 = fig0.add_subplot(111)\n",
    "cax0 = ax0.matshow(daily_sum)\n",
    "fig0.colorbar(cax0)\n",
    "\n",
    "fig1 = pyplot.figure()\n",
    "ax1 = fig1.add_subplot(111)\n",
    "cax1 = ax1.matshow(count_above_one)\n",
    "fig1.colorbar(cax1)\n",
    "\n",
    "\n",
    "fig = pyplot.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(no_value)\n",
    "fig.colorbar(cax)\n",
    "\n",
    "\n",
    "fig2 = pyplot.figure()\n",
    "ax2 = fig2.add_subplot(111)\n",
    "cax2 = ax2.matshow(longest_sequence)\n",
    "fig2.colorbar(cax2)\n",
    "\n",
    "pyplot.show()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
