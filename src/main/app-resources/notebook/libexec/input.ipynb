{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## WFP-01-03-02 CHIRPS Rainfall Estimates (RFE) - Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This application generates Rainfall Estimates (RFE) aggregations, from CHIRPS RFE 5km resolution, compared to a reference period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <a name=\"objective\">Objective \n",
    "\n",
    "The objective of this code is to determine:\n",
    "    - Sum of daily data over the past N days, derived every 10 days (N = 10, 30, 60, 90, 120, 150, 180, 270, 365 days)\n",
    "    - Counts of daily data above 1mm over the past N days, derived every 10 days (N = 30, 60, 90 days).\n",
    "    - Longest sequence of daily values < 2mm (\"dry spell\") within the last N days, derived every 10 days (N = 30, 60, 90 days).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"service\">Service definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = dict([('title', 'CHIRPS Rainfall Estimates (RFE) - Aggregations'),\n",
    "                ('abstract', 'TBD'),\n",
    "                ('id', 'wfp-01-03-02')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"parameter\">Parameter Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_10 = dict([('id', 'N_10'),\n",
    "             ('value', 'True'),\n",
    "             ('title', '10 Day Aggregation'),\n",
    "             ('abstract', 'Get a 10 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "N_30 = dict([('id', 'N_30'),\n",
    "             ('value', 'True'),\n",
    "             ('title', '30 Day Aggregation'),\n",
    "             ('abstract', 'Get a 30 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_60 = dict([('id', 'N_60'),\n",
    "             ('value', 'False'),\n",
    "             ('title', '60 Day Aggregation'),\n",
    "             ('abstract', 'Get a 60 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_90 = dict([('id', 'N_90'),\n",
    "             ('value', 'False'),\n",
    "             ('title', '90 Day Aggregation'),\n",
    "             ('abstract', 'Get a 90 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_120 = dict([('id', 'N_120'),\n",
    "              ('value', 'False'),\n",
    "              ('title', '120 Day Aggregation'),\n",
    "              ('abstract', 'Get a 120 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_150 = dict([('id', 'N_150'),\n",
    "              ('value', 'False'),\n",
    "              ('title', '150 Day Aggregation'),\n",
    "              ('abstract', 'Get a 150 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_180 = dict([('id', 'N_180'),\n",
    "              ('value', 'False'),\n",
    "              ('title', '180 Day Aggregation'),\n",
    "              ('abstract', 'Get a 180 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_270 = dict([('id', 'N_270'),\n",
    "              ('value', 'False'),\n",
    "              ('title', '270 Day Aggregation'),\n",
    "              ('abstract', 'Get a 270 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_365 = dict([('id', 'N_365'),\n",
    "              ('value', 'False'),\n",
    "              ('title', '365 Day Aggregation'),\n",
    "              ('abstract', 'Get a 365 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionOfInterest = dict([('id', 'regionOfInterest'),\n",
    "                         ('value', 'POLYGON((11.5030755518998 -11.1141633706909,41.0343255518998 -11.1141633706909,41.0343255518998 -34.9763656693858,11.5030755518998 -34.9763656693858,11.5030755518998 -11.1141633706909))'),\n",
    "                         ('title', 'WKT Polygon for the Region of Interest (-1 if no crop)'),\n",
    "                         ('abstract', 'Set the value of WKT Polygon')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameOfRegion = dict([('id', 'nameOfRegion'),\n",
    "                     ('value',  'SouthernAfrica'),\n",
    "                     ('title', 'Name of Region'),\n",
    "                     ('abstract', 'Name of the region of interest'),\n",
    "                     ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2process = dict([('id', 'n2process'),\n",
    "                  ('value', '3'),\n",
    "                  ('title', 'n of aggs to process'),\n",
    "                  ('abstract', 'n of aggs to process (-1 to process everthing)')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = dict([('id', 'startdate'),\n",
    "                  ('value', '2016-01-01T00:00Z'),\n",
    "                  ('title', 'Start date'),\n",
    "                  ('abstract', 'Start date')])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enddate = dict([('id', 'enddate'),\n",
    "                ('value', '2016-03-31T23:59Z'),\n",
    "                ('title', 'End date'),\n",
    "                ('abstract', 'End date')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_url = dict([('id', 'catalogue_url'),\n",
    "                      ('value', 'https://catalog.terradue.com/chirps/search'),\n",
    "                      ('title', 'catalogue url for chirps products'),\n",
    "                      ('abstract', 'catalogue url for chirps products')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lta_url = dict([('id', 'lta_url'),\n",
    "                ('value', 'https://catalog.terradue.com//better-wfp-00009/series/results/search'),\n",
    "                ('title', 'Catalogue Url for the LTA products'),\n",
    "                ('abstract', 'Catalogue Url for anomalies')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"runtime\">Runtime parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input references**\n",
    "\n",
    "This is the CHIRPS stack catalogue references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_references = ['https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.01.01','https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.01.02','https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.01.03','https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.01.04','https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.01.05','https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.01.06','https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.01.07','https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.01.08','https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.01.09','https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2017.01.10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_references = ['https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2015.01.'+'{0:02}'.format(i) for i in range(1,31+1)]\n",
    "\n",
    "#for i in range(1,28+1):\n",
    "#    input_references.append('https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2015.02.'+'{0:02}'.format(i))\n",
    "#    \n",
    "#for i in range(1,31+1):\n",
    "#    input_references.append('https://catalog.terradue.com/chirps/search?format=atom&uid=chirps-v2.0.2015.03.'+'{0:02}'.format(i))\n",
    "\n",
    "#input_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"workflow\">Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the packages required for processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from osgeo import gdal, ogr, osr\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.wkt import loads\n",
    "\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import calendar\n",
    "import string\n",
    "\n",
    "import cioppy\n",
    "ciop = cioppy.Cioppy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aux folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = 'tmp_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(temp_folder):\n",
    "    os.mkdir(temp_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_sum(mat1, mat2, no_data_value=None):\n",
    "    if no_data_value is not None:\n",
    "        if not isinstance(mat1, int):\n",
    "            mat1[(mat1 == no_data_value)] = 0\n",
    "        if not isinstance(mat2, int):\n",
    "            mat2[(mat2 == no_data_value)] = 0\n",
    "    return mat1 + mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_matrix(input_mat, threshold_value, greater_than, no_data_value=None):\n",
    "    \n",
    "    if no_data_value is not None:\n",
    "        input_mat[(input_mat == no_data_value)] = -9999.0\n",
    "    if greater_than:\n",
    "        result = np.where(input_mat > threshold_value, 1, 0)\n",
    "    else: \n",
    "        result = np.where((input_mat < threshold_value) & (input_mat >= 0), 1, 0)\n",
    "\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(input_image, polygon_wkt, output_path, product_type=None, do_crop = True):\n",
    "    \n",
    "    dataset = None\n",
    "        \n",
    "    if input_image.startswith('ftp://') or input_image.startswith('http'):\n",
    "        try:\n",
    "            dataset = gdal.Open('/vsigzip//vsicurl/%s' % input_image)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    elif '.nc' in input_image:\n",
    "        dataset = gdal.Open('NETCDF:' + input_image + ':' + product_type)\n",
    "\n",
    "    if do_crop:\n",
    "        polygon_ogr = ogr.CreateGeometryFromWkt(polygon_wkt)\n",
    "        envelope = polygon_ogr.GetEnvelope()\n",
    "        bounds = [envelope[0], envelope[3], envelope[1], envelope[2]]         \n",
    "   \n",
    "        gdal.Translate(output_path, dataset, projWin=bounds, projWinSRS='EPSG:4326')\n",
    "    else:\n",
    "        gdal.Translate(output_path, dataset)\n",
    "        \n",
    "    dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output_image(filepath, output_matrix, image_format, number_of_images, data_format, output_projection=None, output_geotransform=None, mask=None, no_data_value=None):\n",
    "    \n",
    "    driver = gdal.GetDriverByName(image_format)\n",
    "    \n",
    "    out_rows = np.size(output_matrix, 0)\n",
    "    out_columns = np.size(output_matrix, 1)\n",
    "    \n",
    "    output = driver.Create(filepath, out_columns, out_rows, 1, data_format)\n",
    "    \n",
    "    \n",
    "    if mask is not None:\n",
    "            \n",
    "        if no_data_value is not None:\n",
    "            \n",
    "            output_matrix[mask > 0] = no_data_value\n",
    "            output_matrix[mask >= number_of_images] = no_data_value\n",
    "            \n",
    "    if output_projection is not None:\n",
    "        output.SetProjection(output_projection)\n",
    "    if output_geotransform is not None:\n",
    "        output.SetGeoTransform(output_geotransform)\n",
    "    \n",
    "    raster_band = output.GetRasterBand(1)\n",
    "    \n",
    "    if no_data_value is not None:\n",
    "        raster_band.SetNoDataValue(no_data_value)\n",
    "        \n",
    "    raster_band.WriteArray(output_matrix)\n",
    "    \n",
    "    gdal.Warp(filepath, output, format=\"GTiff\", outputBoundsSRS='EPSG:4326')\n",
    "    \n",
    "    output.FlushCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_list(image_list):\n",
    "    mat_list = []\n",
    "    for img in image_list:\n",
    "        dataset = gdal.Open(img)\n",
    "        product_array = dataset.GetRasterBand(1).ReadAsArray()\n",
    "        mat_list.append(product_array)\n",
    "        dataset = None\n",
    "    return mat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(row, search_params):\n",
    "    search = ciop.search(end_point=row['catalogue_url'], \n",
    "                                  params=search_params,\n",
    "                                  output_fields='self,identifier,startdate,enclosure',\n",
    "                                  model='GeoTime')[0]\n",
    "    \n",
    "    series = pd.Series(search)\n",
    "    \n",
    "    series['startdate'] = pd.to_datetime(series['startdate'])\n",
    "    \n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lta_info(row, search_params):\n",
    "    search = ciop.search(end_point=row['catalogue_url'], \n",
    "                                  params=search_params,\n",
    "                                  output_fields='self,identifier,startdate,enclosure,title,enddate,wkt',\n",
    "                                  model='GeoTime')[0]\n",
    "    \n",
    "    series = pd.Series(search)\n",
    "    \n",
    "    series['startdate'] = pd.to_datetime(series['startdate'])\n",
    "    series['enddate'] = pd.to_datetime(series['enddate'])\n",
    "    \n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_aggregations(product_list, N_value, region_of_interest, do_crop = True):\n",
    "    \n",
    "    mask_no_data_value = 0\n",
    "    sum_result = 0\n",
    "    count_above_one = 0\n",
    "    max_sequence = 0\n",
    "    temp_mat = 0\n",
    "    regions_below_two = 0\n",
    "    projection = None\n",
    "    geo_transform = None\n",
    "    \n",
    "    for chirp_product_url in product_list:\n",
    "        \n",
    "        # uncompressed data\n",
    "        chirp_product = (chirp_product_url.split('/')[-1]).split('.gz')[0]\n",
    "        \n",
    "        cropped_product_path = os.path.join(temp_folder, 'crop_' + chirp_product)\n",
    "        \n",
    "        try:\n",
    "            if os.path.isfile(cropped_product_path):\n",
    "                print('already cropped: ' + chirp_product)\n",
    "            else:\n",
    "                if do_crop:\n",
    "                    print('download and crop: ' + chirp_product)\n",
    "                    crop_image(chirp_product_url, region_of_interest, cropped_product_path)\n",
    "                else:\n",
    "                    print('download: ' + chirp_product)\n",
    "                    crop_image(chirp_product_url, region_of_interest, cropped_product_path, None, False)\n",
    "                \n",
    "            # Read GeoTIFF as an array\n",
    "            dataset = gdal.Open(cropped_product_path)\n",
    "            \n",
    "            product_array = (dataset.GetRasterBand(1).ReadAsArray()).astype(float)\n",
    "            \n",
    "            #no_data_value = dataset.GetRasterBand(1).GetNoDataValue()\n",
    "            no_data_value = -9999\n",
    "            geo_transform = dataset.GetGeoTransform()\n",
    "            projection = dataset.GetProjection()\n",
    "            ## Create mask of no_data_values\n",
    "            '''\n",
    "            if isinstance(mask_no_data_value, int):\n",
    "                mask_no_data_value = np.where(product_array == no_data_value, 1, 0)\n",
    "            else:\n",
    "                temp_mask = np.where(product_array == no_data_value, 1, 0)\n",
    "                mask_no_data_value = matrix_sum(mask_no_data_value, temp_mask)\n",
    "            ''' \n",
    "            temp_mask = np.where(product_array == no_data_value, 1, 0)\n",
    "            mask_no_data_value = matrix_sum(mask_no_data_value, temp_mask)\n",
    "\n",
    "            ## Create iteratively the sum array\n",
    "            sum_result = matrix_sum(sum_result, product_array, no_data_value)\n",
    "            \n",
    "            ## Create iteratively the array with the counts of daily data above 1mm\n",
    "            if N_value == 30 or N_value == 60 or N_value == 90:\n",
    "                regions_above_one = mask_matrix(product_array, 1, True, no_data_value)\n",
    "                count_above_one = matrix_sum(count_above_one, regions_above_one)\n",
    "\n",
    "                ## Create iteratively the array with the longest sequence of daily values <2mm\n",
    "                regions_below_two = mask_matrix(product_array, 2, False, no_data_value)\n",
    "                temp_mat = matrix_sum(temp_mat, regions_below_two)\n",
    "                if isinstance(max_sequence, int):\n",
    "                    max_sequence = temp_mat\n",
    "                max_sequence[regions_below_two == 0] = np.maximum(max_sequence[regions_below_two == 0], temp_mat[regions_below_two == 0])\n",
    "                temp_mat[regions_below_two == 0] = 0\n",
    "            \n",
    "            dataset = None\n",
    "\n",
    "            #if os.path.exists(cropped_product_path):\n",
    "            #    os.remove(cropped_product_path)\n",
    "                \n",
    "        except AttributeError as attr_err:\n",
    "            message = 'ERROR reading image. Aborting aggregation. Details: ' + str(attr_err)\n",
    "            print(message)\n",
    "            ciop.log('ERROR', message)\n",
    "            sum_result = None\n",
    "            count_above_one = None\n",
    "            max_sequence = None\n",
    "            mask_no_data_value = None\n",
    "            projection = None\n",
    "            geo_transform = None\n",
    "            break\n",
    "            \n",
    "        if N_value == 30 or N_value == 60 or N_value == 90:\n",
    "            max_sequence[temp_mat != 0] = np.maximum(max_sequence[temp_mat != 0], temp_mat[temp_mat != 0])\n",
    "    \n",
    "    if isinstance(sum_result, int):\n",
    "        sum_result = None\n",
    "    if isinstance(count_above_one, int):\n",
    "        count_above_one = None\n",
    "    if isinstance(max_sequence, int):\n",
    "        max_sequence = None\n",
    "    \n",
    "    return sum_result, count_above_one, max_sequence, mask_no_data_value, projection, geo_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product(url, dest):\n",
    "\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    open(dest, 'wb').write(r.content)\n",
    "    \n",
    "    return r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_outputs(roi_name, first_date, last_date, dekad_flag, sum_result, count_above_one, max_sequence, mask_no_data_value, image_format, product_count, projection, geo_transform, no_data_value):\n",
    "    \n",
    "    image_number = (datetime.strptime(last_date, '%Y-%m-%d') - datetime.strptime(first_date, '%Y-%m-%d')).days\n",
    "    \n",
    "    filenames = []\n",
    "    \n",
    "    filenames.append('CHIRPSv2_' + roi_name + '_N' + str(product_count) + '_daystotal_' + '-'.join(last_date.split('-')[:-1]) + '-' + dekad_flag + '.tif')\n",
    "    \n",
    "    write_output_image(filenames[0], sum_result, image_format, image_number, gdal.GDT_Int16, projection, geo_transform, mask_no_data_value, no_data_value)\n",
    "    \n",
    "    if product_count == 30 or product_count == 60 or product_count == 90:\n",
    "    \n",
    "        filenames.append('CHIRPSv2_' + roi_name + '_N' + str(product_count) + '_countaboveone_' + '-'.join(last_date.split('-')[:-1]) + '-' + dekad_flag + '.tif')\n",
    "        filenames.append('CHIRPSv2_' + roi_name + '_N' + str(product_count) + '_dryspell_' + '-'.join(last_date.split('-')[:-1]) + '-' + dekad_flag + '.tif')\n",
    "        \n",
    "        write_output_image(filenames[1], count_above_one, image_format, image_number, gdal.GDT_Int16, projection, geo_transform, mask_no_data_value, no_data_value)\n",
    "        write_output_image(filenames[2], max_sequence, image_format, image_number, gdal.GDT_Int16, projection, geo_transform, mask_no_data_value, no_data_value)\n",
    "    \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formatted_date(product_row):\n",
    "    '''metadata = ciop.search(end_point=product_reference,\n",
    "                           params=[],\n",
    "                           output_fields='identifier,startdate',\n",
    "                           model=\"GeoTime\")[0]\n",
    "    print(metadata)\n",
    "    '''\n",
    "    date = datetime.strftime(product_row['startdate'], '%Y-%m-%dT00:00:00Z')\n",
    "    \n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_properties_file(dataframe, output_name):\n",
    "    \n",
    "    title = 'Output %s' % output_name\n",
    "    \n",
    "    first_date = get_formatted_date(dataframe.iloc[-1])\n",
    "    last_date = get_formatted_date(dataframe.iloc[0])\n",
    "    \n",
    "    with open(output_name + '.properties', 'wb') as file:\n",
    "        file.write('title=%s\\n' % title)\n",
    "        file.write('date=%s/%s\\n' % (first_date, last_date))\n",
    "        file.write('geometry=%s' % (regionOfInterest['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lta_list(url, search_params):\n",
    "    \n",
    "    search = ciop.search(end_point=url,\n",
    "                         params=search_params,\n",
    "                         output_fields='self,identifier,enclosure,title,startdate,enddate,wkt,updated',\n",
    "                         model='GeoTime')\n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lta_from_dataframe(dataframe, region_name, product_type, N, aggregation, end_day, end_month, wkt):\n",
    "    \n",
    "    region_polygon = loads(wkt)\n",
    "    \n",
    "    dataframe_agr = dataframe[(dataframe['title'].str.contains(aggregation)) &\n",
    "                              (dataframe['title'].str.contains(product_type)) &\n",
    "                              (dataframe['title'].str.contains('_N' + str(N) + '_')) &\n",
    "                              (dataframe['title'].str.contains(region_name))]\n",
    "    \n",
    "    period_lta = dataframe_agr[(dataframe_agr['enddate'].dt.day == end_day) & \n",
    "                     (dataframe_agr['enddate'].dt.month == end_month) &\n",
    "                     (dataframe_agr['wkt'] == region_polygon)]\n",
    "    \n",
    "    if len(period_lta.index.values) > 1:\n",
    "        \n",
    "        outdated_indexes = period_lta[period_lta['updated'] != max(period_lta['updated'])].index.values\n",
    "        \n",
    "        period_lta = period_lta.drop(outdated_indexes)\n",
    "        \n",
    "    return period_lta['enclosure'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly (agg, LTA_agg, end_date, dekad_flag, agg_name, no_value, N, region_of_interest, nameOfRegion, projection, geo_transform):\n",
    "    \n",
    "    LTA_agg = LTA_agg[0]\n",
    "        \n",
    "    start_year = re.findall('\\d{4}', os.path.basename(LTA_agg))[0]\n",
    "    end_year = re.findall('\\d{4}', os.path.basename(LTA_agg))[1]\n",
    "\n",
    "    filepath = os.path.join(temp_folder, os.path.basename(LTA_agg))\n",
    "    \n",
    "    status = 200\n",
    "    if os.path.isfile(filepath):\n",
    "        print('LTA already downloaded: ' + filepath)\n",
    "    else:\n",
    "        print('donwload LTA: ' + filepath)\n",
    "        status = get_product(LTA_agg, filepath)\n",
    "        \n",
    "    if status == 200:\n",
    "            \n",
    "        try:\n",
    "            filepath = [filepath]\n",
    "            LTA_agg = get_matrix_list(filepath)[0]\n",
    "        \n",
    "            anomaly_agg = np.divide(agg, LTA_agg) * 100.0\n",
    "            filename = write_anomaly_output(anomaly_agg, end_date, dekad_flag, start_year, end_year, agg_name, no_value, N, region_of_interest, nameOfRegion, projection, geo_transform, -9999)\n",
    "\n",
    "        except ValueError as e:\n",
    "        \n",
    "            ciop.log('ERROR','Could not calculate anomaly. ' + str(e))\n",
    "                \n",
    "            os.remove(filepath[0])\n",
    "        \n",
    "        return filename\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_anomaly_output(anomaly, last_date, dekad_flag, lta_start_year, lta_end_year, aggregation, mask_no_value, N_value, regionOfInterest, roi_name, projection, geo_transform, no_data_value):\n",
    "    \n",
    "    #image_number = (datetime.strptime(last_date, '%Y-%m-%d') - datetime.strptime(first_date, '%Y-%m-%d')).days\n",
    "    \n",
    "    image_number = N\n",
    "    \n",
    "    filename = 'CHIRPSv2_Anomaly_' + roi_name + '_N' + str(N_value) + '_' + aggregation + '_' + '-'.join(last_date.split('-')[:-1]) + '-' + dekad_flag + '_LTA' + str(lta_start_year) + '_' + str(lta_end_year) + '.tif'\n",
    "    \n",
    "    write_output_image(filename, anomaly.astype('int16'), 'GTiff', image_number, gdal.GDT_Int16, projection, geo_transform, mask_no_value, no_data_value)\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(input_references, str):\n",
    "    input_references = [input_references]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check chosen Ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = [N_10['value'], N_30['value'], N_60['value'], N_90['value'], N_120['value'], N_150['value'], N_180['value'], N_270['value'], N_365['value']]\n",
    "nvalue = [10, 30, 60, 90, 120, 150, 180, 270, 365]\n",
    "nlist = [n == 'True' for n in nlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get chirps input metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd_data = pd.DataFrame(input_references, columns=['catalogue_url'])\n",
    "\n",
    "gpd_data = gpd_data.sort_values(by='catalogue_url')\n",
    "start_date = re.findall('\\d{4}\\.\\d{2}\\.\\d{2}', gpd_data.iloc[0]['catalogue_url'])[0].replace('.', '-')\n",
    "end_date = re.findall('\\d{4}\\.\\d{2}\\.\\d{2}', gpd_data.iloc[-1]['catalogue_url'])[0].replace('.', '-')\n",
    "print(start_date)\n",
    "print(end_date)\n",
    "print(len(input_references))\n",
    "search_params =  dict([('start', start_date),\n",
    "                      ('stop', end_date),\n",
    "                      ('count', len(input_references))])\n",
    "gpd_final = gpd_data.apply(lambda row: get_info(row, search_params), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get LTAs metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lta_params = dict([('start', start_date + 'T00:00:00.0000000Z'),\n",
    "                   ('stop', end_date + 'T00:00:00.0000000Z'),\n",
    "                   ('geom', regionOfInterest['value']),\n",
    "                   ('count', 'unlimited')])\n",
    "print(lta_params)\n",
    "\n",
    "lta_data = get_lta_list(lta_url['value'], lta_params)\n",
    "\n",
    "lta_data = pd.DataFrame.from_dict(lta_data)\n",
    "\n",
    "# only chirp ltas\n",
    "lta_data = lta_data[lta_data['enclosure'].str.contains('CHIRPSv2')]\n",
    "\n",
    "# only chosen Ns\n",
    "cond = None\n",
    "for nv,nl in zip(nvalue,nlist):\n",
    "    if nl:\n",
    "        if cond is None:\n",
    "            cond = lta_data['enclosure'].str.contains('N{0}'.format(nv))\n",
    "        else:\n",
    "            cond = ((cond) | (lta_data['enclosure'].str.contains('N{0}'.format(nv))))\n",
    "        print('N{0}'.format(nv))\n",
    "\n",
    "if cond is not None:\n",
    "    lta_data = lta_data[cond]\n",
    "\n",
    "\n",
    "lta_data['startdate'] = pd.to_datetime(lta_data['startdate'])\n",
    "lta_data['enddate'] = pd.to_datetime(lta_data['enddate'])\n",
    "lta_data['wkt'] = lta_data['wkt'].apply(lambda row: loads(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lta_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update AOI if crop not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_crop = True\n",
    "\n",
    "if regionOfInterest['value'] == '-1':\n",
    "    \n",
    "    dataset = gdal.Open('/vsigzip//vsicurl/%s' % gpd_final.iloc[0]['enclosure'])\n",
    "    \n",
    "    geoTransform = dataset.GetGeoTransform()\n",
    "    \n",
    "    minx = geoTransform[0]\n",
    "    maxy = geoTransform[3]\n",
    "    maxx = minx + geoTransform[1] * dataset.RasterXSize\n",
    "    miny = maxy + geoTransform[5] * dataset.RasterYSize\n",
    "    \n",
    "    regionOfInterest['value'] = 'POLYGON(({0} {1}, {2} {1}, {2} {3}, {0} {3}, {0} {1}))'.format(minx, maxy, maxx, miny)\n",
    "    \n",
    "    do_crop = False\n",
    "    \n",
    "    dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameOfRegion = nameOfRegion['value']\n",
    "region_of_interest = regionOfInterest['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sort chirp references by startdate (descending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prods = gpd_final.sort_values(by='startdate', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify benchmarks (dekads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uyears = prods['startdate'].dt.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_by_year = []\n",
    "for uy in uyears:\n",
    "    months_by_year.append( (uy, prods[prods['startdate'].dt.year == uy]['startdate'].dt.month.unique().tolist()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = []\n",
    "for my in months_by_year:\n",
    "    dom1st = 10\n",
    "    dom20th = 20\n",
    "    for m in my[1]:\n",
    "        domlast = calendar.monthrange(my[0], m)[-1]\n",
    "        benchmarks.append(datetime(my[0],m,domlast))\n",
    "        benchmarks.append(datetime(my[0],m,dom20th))\n",
    "        benchmarks.append(datetime(my[0],m,dom1st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = pd.Series(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute aggregations and anomalies (if LTAs available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nv,nl in zip(nvalue,nlist):\n",
    "    \n",
    "    if nl:\n",
    "        \n",
    "        N = nv\n",
    "    \n",
    "        processing_index = 0\n",
    "        for index, value in benchmarks.items():\n",
    "\n",
    "            print('Index : {0}, Value : {1}'.format(index,value))\n",
    "            \n",
    "            dekad_flag = 'd3'\n",
    "        \n",
    "            if value.day == 10:\n",
    "                dekad_flag = 'd1'\n",
    "            elif value.day == 20:\n",
    "                dekad_flag = 'd2'\n",
    "    \n",
    "            prodsbin = prods[prods['startdate'].between(value - timedelta(days=N-1), value, inclusive=True)]\n",
    "    \n",
    "            \n",
    "            if len(prodsbin) != N: # if the number of products don't match N\n",
    "                continue\n",
    "                \n",
    "            if len(prodsbin) > 0:\n",
    "                processing_index = processing_index + 1\n",
    "                \n",
    "        \n",
    "            if int(n2process['value']) == -1:\n",
    "                pass\n",
    "            else:\n",
    "                if processing_index-1 == int(n2process['value']):\n",
    "                    break\n",
    "        \n",
    "\n",
    "            #print(len(prodsbin))\n",
    "            #print(prodsbin['startdate'])\n",
    "    \n",
    "            start_date = prodsbin['startdate'].iloc[-1]\n",
    "            start_date = start_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "            end_date = prodsbin['startdate'].iloc[0]\n",
    "            end_day = end_date.day\n",
    "            end_month = end_date.month    \n",
    "            end_date = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    \n",
    "            interval_products = prodsbin['enclosure'].tolist()\n",
    "        \n",
    "            # compute aggregation\n",
    "            daily_sum, count_above_one, longest_sequence, no_value, projection, geo_transform = calc_aggregations(interval_products, N, region_of_interest, do_crop)\n",
    "    \n",
    "    \n",
    "            if daily_sum is not None:\n",
    "                \n",
    "                # save agg to tif\n",
    "                filenames = write_outputs(nameOfRegion, start_date, end_date, dekad_flag, daily_sum, count_above_one, longest_sequence, no_value, 'GTiff', N, projection, geo_transform, -9999)\n",
    "                \n",
    "                ## ANOMALY - daily_sum ##\n",
    "                # verify if LTA is available\n",
    "                LTA_daily_sum = get_lta_from_dataframe(lta_data, nameOfRegion, 'CHIRPSv2', N, 'daystotal', end_day, end_month, region_of_interest)\n",
    "            \n",
    "                # if LTA available compute anomaly\n",
    "                if len(LTA_daily_sum) > 0:\n",
    "            \n",
    "                    print('+----------+ LTA +----------------+')\n",
    "                    print([os.path.basename(l) for l in LTA_daily_sum])\n",
    "                    print('+----------+ LTA +----------------+')\n",
    "                    \n",
    "                    anom_filename = compute_anomaly (daily_sum, LTA_daily_sum, end_date, dekad_flag, 'daystotal', no_value, N, region_of_interest, nameOfRegion, projection, geo_transform)\n",
    "\n",
    "                    filenames.append(anom_filename)\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    period_str = str(end_month) + '-' + str(end_day)\n",
    "                    message = 'No Long-term Average match found for \"daystotal\" aggregation from the period (month-day): ' + period_str \n",
    "                    ciop.log('INFO', message)\n",
    "                    \n",
    "                    \n",
    "                if count_above_one is not None:\n",
    "                    \n",
    "                    ## ANOMALY - count_above_one ##\n",
    "                    # verify if LTA is available\n",
    "                    LTA_count_above_one = get_lta_from_dataframe(lta_data, nameOfRegion, 'CHIRPSv2', N, 'countaboveone', end_day, end_month, region_of_interest)\n",
    "                    \n",
    "                    # if LTA available compute anomaly\n",
    "                    if len(LTA_count_above_one) > 0:\n",
    "            \n",
    "                        print('+----------+ LTA +----------------+')\n",
    "                        print([os.path.basename(l) for l in LTA_count_above_one])\n",
    "                        print('+----------+ LTA +----------------+')\n",
    "                    \n",
    "                        anom_filename = compute_anomaly (count_above_one, LTA_count_above_one, end_date, dekad_flag, 'countaboveone', no_value, N, region_of_interest, nameOfRegion, projection, geo_transform)\n",
    "\n",
    "                        filenames.append(anom_filename)\n",
    "\n",
    "                    else:\n",
    "                        period_str = str(end_month) + '-' + str(end_day)\n",
    "                        message = 'No Long-term Average match found for \"countaboveone\" aggregation from the period (month-day): ' + period_str \n",
    "                        ciop.log('INFO', message)\n",
    "                  \n",
    "                # create properties files for aggregations and anomalies\n",
    "                for output_name in filenames:\n",
    "                    write_properties_file(prodsbin, output_name)\n",
    "                    \n",
    "                print('********************************************')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove temporay files and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.rmtree(temp_folder)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s : %s\" % (temp_folder, e.strerror))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ewf-wfp-01-03-02_02",
   "language": "python",
   "name": "env_ewf-wfp-01-03-02_02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
